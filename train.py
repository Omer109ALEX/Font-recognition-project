"""
This program creates and trains (CNN) models for classifying the fonts of characters in images.
The input to the models is a 64x64 pixels concentration of individual character images,
generated by the 'concentrateImg' function in the 'dataPrepper.py' file.

The output is a 'num_of_fonts'-vector of probabilities, ranging from 0 to 1,
indicating the likelihood of each of the fonts (defined in 'font_index_map' in 'dataPrepper.py') appearing in the image.

The fitting and validation of the models are performed using the dataset in the 'ReadySynthText.h5' HDF5 file,
generated by the 'dataPrepper' program. To classify the fonts of different characters,
separate models are trained and stored in the local 'models' directory,
named after the ASCII values of the corresponding characters in the TensorFlow SavedModel format.

In addition to these character-specific models, a global model trained on the entire dataset
is stored in the 'models' directory as 'global', which can be used for classifying
characters not present in the original train set.
All the individual character models are fine-tuned versions of the global model.

An individual character model will not be saved if
 > there are less than 30 training examples
 > or if its validation loss is lower than that of the global model for the same validation set.

Finally, an 'weights.h5' HDF5 file is created in the 'models' directory, containing one value datasets named
after the ASCII values of characters with corresponding models, along with a 'global' entry for the global model.
The weights are calculated using the formula: (Accuracy / (2-Accuracy))
"""

from utils import num_of_fonts, PROJECT_PATH, NUM_OF_GLOBAL_EPOCHS, NUM_OF_INDIVIDUAL_EPOCHS, MIN_FOR_INDIVIDUAL_MODEL
from keras.regularizers import l2
import tensorflow as tf
from tensorflow.image import random_contrast, random_brightness, random_saturation, random_hue
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, InputLayer
from tensorflow.keras.models import load_model
from tensorflow.keras.backend import clear_session as keras_clear_session
from tensorflow.keras.callbacks import ReduceLROnPlateau
import gc
import os
import h5py
import absl.logging

absl.logging.set_verbosity(absl.logging.ERROR)



class RandomColorDistortion(tf.keras.layers.Layer):
    """
    class RandomColorDistortion : a custom CNN layer for producing random-colored variants
        of the train examples in the dataset, before each epoch of the fitting process.
    """

    def __init__(self, contrast_range=(0, 2), brighness_range=(0.5, 1.5), saturation_range=(0.5, 1.5), hue_delta=0.5,
                 **kwargs):
        """
        Constructor : creates a random color distortion CNN layer, producing random contrast,
            brightness and saturation in the given ranges,c and random hue between -'hue_delta'
            and 'hue_delta'.
        """

        super(RandomColorDistortion, self).__init__(**kwargs)
        self.contrast_range = contrast_range
        self.brighness_range = brighness_range
        self.saturation_range = saturation_range
        self.hue_delta = hue_delta

    def call(self, images, train=None):
        """
        call : distorts the color of the given images (if 'train' is True).
        """
        if train:
            images = random_contrast(images, self.contrast_range[0], self.contrast_range[1])
            images = random_brightness(images, self.brighness_range[0], self.brighness_range[1])
            images = random_saturation(images, self.saturation_range[0], self.saturation_range[1])
            images = random_hue(images, self.hue_delta)
            images = tf.clip_by_value(images, 0, 1)
        return images


def createModel():
    """
    createModel : creates and compiles a new CNN model suited for classifying fonts in
    concentrated images of characters.
    """

    # the architecture I developed:
    model = Sequential([
        InputLayer(input_shape=(64, 64, 3)),
        RandomColorDistortion(name='randomcolordistortion'),

        Conv2D(filters=64, kernel_size=(9, 9), kernel_regularizer=l2(0.0005),
               activation=tf.keras.layers.LeakyReLU(alpha=0.5), padding='same'),
        MaxPooling2D(pool_size=(2, 2)),

        Conv2D(filters=32, kernel_size=(6, 6), activation=tf.keras.layers.LeakyReLU(alpha=0.1)),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        Conv2D(filters=64, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1)),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        # Flatten the results to feed into a DNN
        Flatten(),
        Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),
        Dropout(0.25),
        Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),
        Dropout(0.25),

        # Classification layer
        Dense(num_of_fonts, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])

    print(model.summary())
    return model


def train_main():
    """
    Train_main : create, fit, and store the models described at the top of the file.
    Requirements: the 'ReadySynthText.h5' file produced by 'dataPrepper.py', being in the PROJECT_PATH.
    Produces: 'models' directory, and all its contents described at the top of this file.
    """

    # create the 'models' directory (if it does not exist)
    if not os.path.exists(PROJECT_PATH + 'models'):
        os.mkdir(PROJECT_PATH + 'models')

    # create the HDF5 file 'weights.h5' mentioned above
    with h5py.File(PROJECT_PATH + 'models/weights.h5', 'w') as weights_file:
        # load the dataset
        with h5py.File(PROJECT_PATH + 'ReadySynthText.h5', 'r') as db:

            # first - we fit a global model
            # import the whole dataset
            X_train = db['train']['data']['images'][()]
            y_train = db['train']['data']['font_index'][()]
            X_val = db['val']['data']['images'][()]
            y_val = db['val']['data']['font_index'][()]

            # create, compile, fit, and save the model
            print('Fitting global model')
            global_model = createModel()
            lr_decay = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=10, mode="auto",
                                         verbose=1, min_delta=0.0001, cooldown=0, min_lr=0)

            global_model.fit(x=X_train, y=y_train, epochs=NUM_OF_GLOBAL_EPOCHS, shuffle=True,
                             validation_data=(X_val, y_val), callbacks=[lr_decay])

            global_model.save(PROJECT_PATH + 'models/global')

            # calculate and store its weight
            print('Evaluating global model')
            loss, acc = global_model.evaluate(X_val, y_val)
            weights_file.create_dataset('global', data=(acc / (2 - acc)))

            # cleanup
            keras_clear_session()
            gc.collect()

            # create a model for every character in the train and validation sets
            # characters we are going to examine
            chars = [char for char in db['train']['indexing']['by_char'].keys()
                     if char in db['val']['indexing']['by_char'].keys()]

            for char_i, char in enumerate(chars):
                print('Trying to fit model for character:', char, '|', char_i + 1, '/', len(chars))

                # load train and validation sets
                train_indices = db['train']['indexing']['by_char'][char][()]
                X_train = db['train']['data']['images'][train_indices]
                y_train = db['train']['data']['font_index'][train_indices]
                val_indices = db['val']['indexing']['by_char'][char][()]
                X_val = db['val']['data']['images'][val_indices]
                y_val = db['val']['data']['font_index'][val_indices]

                if len(y_train) >= MIN_FOR_INDIVIDUAL_MODEL:  # we fit a model only when there are at least 20 examples

                    curr_model = load_model(
                        PROJECT_PATH + 'models/global')  # new models are fine-tuned versions of the global one
                    curr_model.compile(optimizer=tf.keras.optimizers.Adam(0.00025),
                                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                                       metrics=['accuracy'])

                    lr_decay = ReduceLROnPlateau(monitor="val_loss", factor=0.5 ** 0.5, patience=10, mode="auto",
                                                 verbose=1, min_delta=0.0001, cooldown=0, min_lr=0)

                    curr_model.fit(x=X_train, y=y_train, epochs=NUM_OF_INDIVIDUAL_EPOCHS, shuffle=True,
                                   validation_data=(X_val, y_val), callbacks=[lr_decay])

                    # evaluate the resulting model and the global model on validation set
                    print('Evaluating model for character:', char, '|', char_i + 1, '/', len(chars))
                    local_loss, local_acc = curr_model.evaluate(X_val, y_val)
                    global_loss, global_acc = global_model.evaluate(X_val, y_val)

                    if local_loss < global_loss:  # we save a model only if it improved over the global one
                        curr_model.save(PROJECT_PATH + 'models/' + char)
                        weights_file.create_dataset(char, data=(local_acc / (2 - local_acc)))

                    # cleanup
                    keras_clear_session()
                    del curr_model
                    gc.collect()

                else:
                    print('Skipped (not enough train examples).')
